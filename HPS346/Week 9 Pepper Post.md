# Where does the problem with AI lie?

In the reading by Sparrow and Hatherley they say this:

*"The factors that have led to the decline in human contact in medicine are economic—which is to say, ultimately political—and technological change alone is unlikely to reverse this" (16)*

This made me think about a common theme in the discussion of the implementation of AI technology. I primarily see arguments coming from two camps

1. The problem with AI is that it is not advanced enough. If our AI systems were highly accurate, reliable, and unbiased, then their implementation and widespread adoption would be a foregone conclusion. 
2. The problems with AI are those of the society it is implemented in. Our economic and political incentives and systematic inequalities mean that AI implementation would do more harm than good. AI trains on the data of society. It will inevitably reflect societal biases no matter how advanced the technology is.

As with most things, the answer likely lies somewhere in the middle of this false dichotomy. It also will be different for each field. For example, I can see point 1 being more applicable for self-driving vehicles, where the data is less likely to be biased. On the other hand, medicine might be plagued more by point 2, as Sparrow and Hatherley argue. 

As always, I would love to hear the class's thoughts on where the problem with AI lies on this spectrum.

I'm not sure how much it has to do with permeance as much as consciousness and the capacity to suffer. GTP-3 can do things that dogs (and I) cannot. However, as far as I know, there is no evidence that it is conscious or feels pain in any way we understand (granted, we don't really understand anything about conciousness). Even if we don't understand suffering or conciousness, I can at least claim this with some certainty: dogs appear to suffer but GTP-3 does not. 

Also, I am not sure if permeance makes things more or less precious. If I knew that when I shut down GTP-3 I would never be able to use it again, I feel like I would be far less likely to "kill it". On the other hand, If I knew that no matter what happend to a dog, it would be fine, I might treat it with less care.